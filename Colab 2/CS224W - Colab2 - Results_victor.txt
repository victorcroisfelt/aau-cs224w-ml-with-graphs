As an optional question, it is suggested to use different polling methods to 
aggregate the informantion along the batches of graphs. These were the results
that I obtained:

global_mean_pool
Epoch: 30, Loss: 0.0396, Train: 80.56%, Valid: 79.83% Test: 72.83%

global_add_pool
Epoch: 30, Loss: 0.0361, Train: 83.25%, Valid: 78.56% Test: 76.51%

global_max_pool
Epoch: 30, Loss: 0.0989, Train: 85.57%, Valid: 79.09% Test: 75.34%

The best one was using the average aggregation. This means that each graph contributes
equally to the prediction and the normalization improves learning.